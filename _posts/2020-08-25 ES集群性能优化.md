#ES集群服务器CPU负载瞬间飚高分析
〉https://my.oschina.net/wellben/blog/1509569

##  Elasticsearch 集群优化-尽可能全面详细
〉https://www.cnblogs.com/passzhang/p/12666271.html

## Elasticsearch 节点磁盘使用率过高，导致ES集群索引无副本
〉https://www.cnblogs.com/operationhome/p/12150530.html

## elasticsearch 性能监控基础
〉https://www.cnblogs.com/smail-bao/p/7448392.html

## 系统级性能分析工具perf的介绍与使用
〉http://blog.itpub.net/24585765/viewspace-2564885/

## 牛逼的Linux性能剖析—perf 
> https://www.cnblogs.com/sky-heaven/p/13151108.html

## load过高排查
```
jstack

线程的运行情况、线程的状态

load过高的排查

jps -mlv 列出所有jvm进程，选择要查的那个，记下pid
top -H -p pid 查出哪个线程占用cpu过高，取该线程的threadid
threadid转成16进制tid  linux命令行下 >  printf "%x" num
jstack tid | grep -A 100 取该线程栈信息的后100行，然后分析信息
 

下面说明一下dump的信息


"pool-18-thread-10" prio=10 tid=0x00007f673807e000 nid=0x24d8 waiting on condition [0x00007f671b971000]
 

waiting on condition   等待某个条件的发生，具体的原因可以查看stack的信息，一般是网络的io，出现大量的waiting on condition，可能是带宽不够，或是网络存在异常
waitint for monitor entry  进入同步块内的线程
Object.wait  阻塞在同步块的线程


既然官方给的解释就是R状态和D状态的平均进程数，那么我就想着是否可以从这两个状态入手呢？我们不妨打印出R状态和D状态的进程数。

ps -eTo stat,pid,tid,ppid,comm --no-header | sed -e 's/^ \*//' | perl -nE 'chomp;say if (m!^\S*[RD]+\S*!)'

通过以上命令可以打印出R状态和D状态的进程数

从这里可以看到有大量的处在D状态的线程，而这些都属于16231的java进程。既然找到导致服务器进程高的进程，我还想进一步追查下什么原因导致的大量线程处于D状态。
可以通过jstack -p 16231
```
